{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "authorship_tag": "ABX9TyPfxtJgE4WQlSNQ8km1vg2D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glnrzr/TezCalismasi/blob/master/japon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPZ_EE1z_X-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.layers import Dense, Dot, Embedding, Input, Reshape\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.sequence import skipgrams\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "np.random.seed(777)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUF_IT1-_z_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing_corpus(corpus, sampling_rate=1.0):\n",
        "    if sampling_rate is not 1.0:\n",
        "        corpus = corpus.sample(frac=sampling_rate, replace=False)\n",
        "    corpus = corpus.str.lower()\n",
        "    corpus = corpus.str.replace(r'[^a-z0-9\\s]', ' ', regex=True)\n",
        "    return corpus.values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9_Cnii0_0EX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def making_vocab(corpus, top_n_ratio=1.0):\n",
        "    words = np.concatenate(np.core.defchararray.split(corpus)).tolist()\n",
        "\n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stopWords]\n",
        "\n",
        "    counter = Counter(words)\n",
        "    if top_n_ratio is not 1.0:\n",
        "        counter = Counter(dict(counter.most_common(int(top_n_ratio*len(counter)))))\n",
        "    unique_words = list(counter) + ['UNK']\n",
        "    return unique_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAvFIoTD_0H1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vocab_indexing(vocab):\n",
        "    word2index = {word:index for index, word in enumerate(vocab)}\n",
        "    index2word = {index:word for word, index in word2index.items()}\n",
        "    return word2index, index2word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F22fE25B_0DP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_index_into_corpus(word2index, corpus):\n",
        "    indexed_corpus = []\n",
        "    for doc in corpus:\n",
        "        indexed_corpus.append([word2index[word] if word in word2index else word2index['UNK'] for word in doc.split()])\n",
        "    return indexed_corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv3ISmsyAHma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generating_wordpairs(indexed_corpus, vocab_size, window_size=4):\n",
        "    X = []\n",
        "    Y = []\n",
        "    for row in indexed_corpus:\n",
        "        x, y = skipgrams(sequence=row, vocabulary_size=vocab_size, window_size=window_size,\n",
        "                        negative_samples=1.0, shuffle=True, categorical=False, sampling_table=None, seed=None)\n",
        "        X = X + list(x)\n",
        "        Y = Y + list(y)\n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBz_m1juAd7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def consructing_model(vocab_size, embedding_dim=300):\n",
        "    input_target = Input((1,))\n",
        "    input_context = Input((1,))\n",
        "\n",
        "    embedding_layer = Embedding(vocab_size, embedding_dim, input_length=1)\n",
        "\n",
        "    target_embedding = embedding_layer(input_target)\n",
        "    target_embedding = Reshape((embedding_dim, 1))(target_embedding)\n",
        "    context_embedding = embedding_layer(input_context)\n",
        "    context_embedding = Reshape((embedding_dim, 1))(context_embedding)\n",
        "\n",
        "    hidden_layer = Dot(axes=1)([target_embedding, context_embedding])\n",
        "    hidden_layer = Reshape((1,))(hidden_layer)\n",
        "\n",
        "    output = Dense(16, activation='sigmoid')(hidden_layer)\n",
        "    output = Dense(1, activation='sigmoid')(output)\n",
        "    \n",
        "    model = Model(inputs=[input_target, input_context], outputs=output)\n",
        "    #model.summary()\n",
        "    model.compile(loss='binary_crossentropy', optimizer='sgd')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJlCPY99AjBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_model(model, epochs, batch_size, indexed_corpus, vocab_size):\n",
        "    for i in range(epochs):\n",
        "        idx_batch = np.random.choice(len(indexed_corpus), batch_size)\n",
        "        X, Y = generating_wordpairs(np.array(indexed_corpus)[idx_batch].tolist(), vocab_size)\n",
        "\n",
        "        word_target, word_context = zip(*X)\n",
        "        word_target = np.array(word_target, dtype=np.int32)\n",
        "        word_context = np.array(word_context, dtype=np.int32)\n",
        "\n",
        "        target = np.zeros((1,))\n",
        "        context = np.zeros((1,))\n",
        "        label = np.zeros((1,))\n",
        "        idx = np.random.randint(0, len(Y)-1)\n",
        "        target[0,] = word_target[idx]\n",
        "        context[0,] = word_context[idx]\n",
        "        label[0,] = Y[idx]\n",
        "        loss = model.train_on_batch([target, context], label)\n",
        "        if i % 1000 == 0:\n",
        "            print(\"Iteration {}, loss={}\".format(i, loss))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np_bggSQAd6s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "63e059da-53ac-4a5e-ef98-0a37b58e1f6f"
      },
      "source": [
        "def save_vectors(file_path, vocab_size, embedding_dim, model, word2index):\n",
        "    f = open(file_path, 'w')\n",
        "    f.write('{} {}\\n'.format(vocab_size-1, embedding_dim))\n",
        "    vectors = model.get_weights()[0]\n",
        "    for word, i in word2index.items():\n",
        "        f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
        "    f.close()\n",
        "    return file_path\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "    time_start = time()\n",
        "    time_check = time()\n",
        "    \n",
        "    corpus = pd.read_csv(\"abcnews-date-text.csv\").iloc[:,1] \n",
        "    corpus = preprocessing_corpus(corpus, sampling_rate=1.0)\n",
        "    print(\"Corpus was loaded in\\t{time} sec\".format(time=time()-time_check)); time_check = time()\n",
        "    \n",
        "    vocab = making_vocab(corpus, top_n_ratio=0.8)\n",
        "    vocab_size = len(vocab)\n",
        "    print(\"Vocabulary was made in\\t{time} sec\".format(time=time()-time_check)); time_check = time()\n",
        "    \n",
        "    word2index, index2word = vocab_indexing(vocab)\n",
        "    print(\"Vocabulary was indexed in\\t{time} sec\".format(time=time()-time_check)); time_check = time()\n",
        "    \n",
        "    indexed_corpus = word_index_into_corpus(word2index, corpus)\n",
        "    print(\"Corpus was indexed in\\t{time} sec\".format(time=time()-time_check)); time_check = time()\n",
        "\n",
        "    embedding_dim = 100\n",
        "    model = consructing_model(vocab_size, embedding_dim=embedding_dim)\n",
        "    print(\"Model was constructed in\\t{time} sec\".format(time=time()-time_check)); time_check = time()\n",
        "\n",
        "    epochs = 100001\n",
        "    batch_sentence_size = 512\n",
        "    model = training_model(model, epochs, 512, indexed_corpus, vocab_size)\n",
        "    print(\"Traning was done in\\t{time} sec\".format(time=time()-time_check)); time_check = time()\n",
        "\n",
        "    save_path = save_vectors('vectors_on_batch.txt', vocab_size, embedding_dim, model, word2index)\n",
        "    print(\"Trained vector was saved in\\t{time} sec\".format(time=time()-time_check)); time_check = time()\n",
        "\n",
        "    print(\"Done: overall process consumes\\t{time} sec\".format(time=time()-time_start))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-e8a878877e53>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    time_start = time()\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1Ybl18-BA6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}